{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this tutorial, we provide an example of creaing a new SSL algorithm by resusing the component hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semilearn import get_dataset, get_data_loader, get_net_builder, get_algorithm, get_config, Trainer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Create and Register Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a new algorithm based on FixMatch and combine it with entropy loss, using the following steps:\n",
    "\n",
    "* Inherit FixMatch algorithm\n",
    "* Rewrite the 'train_step' function by adding entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from semilearn.core.utils import ALGORITHMS\n",
    "from semilearn.algorithms.fixmatch import FixMatch\n",
    "from inspect import signature\n",
    "\n",
    "\n",
    "\n",
    "def entropy_loss(ul_y):\n",
    "    p = F.softmax(ul_y, dim=1)\n",
    "    return -(p * F.log_softmax(ul_y, dim=1)).sum(dim=1).mean(dim=0)\n",
    "\n",
    "\n",
    "@ALGORITHMS.register('fixmatch_entropy')\n",
    "class FixMatchEntropy(FixMatch):\n",
    "    def train_step(self, idx_lb, x_lb, y_lb, x_ulb_w, x_ulb_s):\n",
    "        num_lb = y_lb.shape[0]\n",
    "\n",
    "        # inference and calculate sup/unsup losses\n",
    "        with self.amp_cm():\n",
    "            if self.use_cat:\n",
    "                inputs = torch.cat((x_lb, x_ulb_w, x_ulb_s))\n",
    "                outputs = self.model(inputs)\n",
    "                logits_x_lb = outputs['logits'][:num_lb]\n",
    "                logits_x_ulb_w, logits_x_ulb_s = outputs['logits'][num_lb:].chunk(2)\n",
    "                feats_x_lb = outputs['feat'][:num_lb]\n",
    "                feats_x_ulb_w, feats_x_ulb_s = outputs['feat'][num_lb:].chunk(2)\n",
    "            else:\n",
    "                outs_x_lb = self.model(x_lb) \n",
    "                logits_x_lb = outs_x_lb['logits']\n",
    "                feats_x_lb = outs_x_lb['feat']\n",
    "                outs_x_ulb_s = self.model(x_ulb_s)\n",
    "                logits_x_ulb_s = outs_x_ulb_s['logits']\n",
    "                feats_x_ulb_s = outs_x_ulb_s['feat']\n",
    "                with torch.no_grad():\n",
    "                    outs_x_ulb_w = self.model(x_ulb_w)\n",
    "                    logits_x_ulb_w = outs_x_ulb_w['logits']\n",
    "                    feats_x_ulb_w = outs_x_ulb_w['feat']\n",
    "            feat_dict = {'x_lb':feats_x_lb, 'x_ulb_w':feats_x_ulb_w, 'x_ulb_s':feats_x_ulb_s}\n",
    "\n",
    "            sup_loss = self.ce_loss(logits_x_lb, y_lb, reduction='mean')\n",
    "            \n",
    "            # probs_x_ulb_w = torch.softmax(logits_x_ulb_w, dim=-1)\n",
    "            probs_x_ulb_w = self.compute_prob(logits_x_ulb_w.detach())\n",
    "            \n",
    "            # if distribution alignment hook is registered, call it \n",
    "            # this is implemented for imbalanced algorithm - CReST\n",
    "            if self.registered_hook(\"DistAlignHook\"):\n",
    "                probs_x_ulb_w = self.call_hook(\"dist_align\", \"DistAlignHook\", probs_x_ulb=probs_x_ulb_w.detach())\n",
    "\n",
    "            # compute mask\n",
    "            mask = self.call_hook(\"masking\", \"MaskingHook\", logits_x_ulb=probs_x_ulb_w, softmax_x_ulb=False)\n",
    "\n",
    "            # generate unlabeled targets using pseudo label hook\n",
    "            pseudo_label = self.call_hook(\"gen_ulb_targets\", \"PseudoLabelingHook\", \n",
    "                                          logits=probs_x_ulb_w,\n",
    "                                          use_hard_label=self.use_hard_label,\n",
    "                                          T=self.T,\n",
    "                                          softmax=False)\n",
    "\n",
    "            unsup_loss = self.consistency_loss(logits_x_ulb_s,\n",
    "                                               pseudo_label,\n",
    "                                               'ce',\n",
    "                                               mask=mask)\n",
    "            \n",
    "            # NOTE: add entropy loss here\n",
    "            loss_entmin = entropy_loss(logits_x_ulb_w)\n",
    "\n",
    "            total_loss = sup_loss + self.lambda_u * unsup_loss\n",
    "\n",
    "        out_dict = self.process_out_dict(loss=total_loss, feat=feat_dict)\n",
    "        log_dict = self.process_log_dict(sup_loss=sup_loss.item(), \n",
    "                                         unsup_loss=unsup_loss.item(), \n",
    "                                         total_loss=total_loss.item(), \n",
    "                                         util_ratio=mask.float().mean().item())\n",
    "        return out_dict, log_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: define configs and create config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: netstat: not found\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'algorithm': 'fixmatch_entropy',\n",
    "    'net': 'vit_tiny_patch2_32',\n",
    "    'use_pretrain': True, \n",
    "    'pretrain_path': 'https://github.com/microsoft/Semi-supervised-learning/releases/download/v.0.0.0/vit_tiny_patch2_32_mlp_im_1k_32.pth',\n",
    "\n",
    "    # optimization configs\n",
    "    'epoch': 1,  # set to 100\n",
    "    'num_train_iter': 5000,  # set to 102400\n",
    "    'num_eval_iter': 500,   # set to 1024\n",
    "    'num_log_iter': 50,    # set to 256\n",
    "    'optim': 'AdamW',\n",
    "    'lr': 5e-4,\n",
    "    'layer_decay': 0.5,\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': 16,\n",
    "\n",
    "\n",
    "    # dataset configs\n",
    "    'dataset': 'cifar10',\n",
    "    'num_labels': 40,\n",
    "    'num_classes': 10,\n",
    "    'img_size': 32,\n",
    "    'crop_ratio': 0.875,\n",
    "    'data_dir': './data',\n",
    "    'ulb_samples_per_class': None,\n",
    "\n",
    "    # algorithm specific configs\n",
    "    'hard_label': True,\n",
    "    'uratio': 2,\n",
    "    'ulb_loss_ratio': 1.0,\n",
    "\n",
    "    # device configs\n",
    "    'gpu': 0,\n",
    "    'world_size': 1,\n",
    "    'distributed': False,\n",
    "    \"num_workers\": 2,\n",
    "}\n",
    "config = get_config(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: create algorithm and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "lb count: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "ulb count: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "Files already downloaded and verified\n",
      "unlabeled data number: 50000, labeled data number 40\n",
      "Create train and test data loaders\n",
      "[!] data loader keys: dict_keys(['train_lb', 'train_ulb', 'eval'])\n",
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n",
      "Create optimizer and scheduler\n"
     ]
    }
   ],
   "source": [
    "algorithm = get_algorithm(config,  get_net_builder(config.net, from_name=False), tb_log=None, logger=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "lb count: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "ulb count: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = get_dataset(config, config.algorithm, config.dataset, config.num_labels, config.num_classes, data_dir=config.data_dir, include_lb_to_ulb=config.include_lb_to_ulb)\n",
    "train_lb_loader = get_data_loader(config, dataset_dict['train_lb'], config.batch_size)\n",
    "train_ulb_loader = get_data_loader(config, dataset_dict['train_ulb'], int(config.batch_size * config.uratio))\n",
    "eval_loader = get_data_loader(config, dataset_dict['eval'], config.eval_batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haoc/miniconda3/envs/test/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[2022-12-18 16:02:52,476 INFO] confusion matrix\n",
      "[2022-12-18 16:02:52,478 INFO] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "[2022-12-18 16:02:52,481 INFO] evaluation metric\n",
      "[2022-12-18 16:02:52,483 INFO] acc: 0.1000\n",
      "[2022-12-18 16:02:52,484 INFO] precision: 0.0100\n",
      "[2022-12-18 16:02:52,485 INFO] recall: 0.1000\n",
      "[2022-12-18 16:02:52,486 INFO] f1: 0.0182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved: ./saved_models/fixmatch/latest_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-18 16:02:53,467 INFO] Best acc 0.1000 at epoch 0\n",
      "[2022-12-18 16:02:53,468 INFO] Training finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved: ./saved_models/fixmatch/model_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haoc/miniconda3/envs/test/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[2022-12-18 16:04:06,346 INFO] confusion matrix\n",
      "[2022-12-18 16:04:06,348 INFO] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "[2022-12-18 16:04:06,351 INFO] evaluation metric\n",
      "[2022-12-18 16:04:06,352 INFO] acc: 0.1000\n",
      "[2022-12-18 16:04:06,353 INFO] precision: 0.0100\n",
      "[2022-12-18 16:04:06,354 INFO] recall: 0.1000\n",
      "[2022-12-18 16:04:06,354 INFO] f1: 0.0182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.1, 'precision': 0.01, 'recall': 0.1, 'f1': 0.01818181818181818}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training and evaluation\n",
    "trainer = Trainer(config, algorithm)\n",
    "trainer.fit(train_lb_loader, train_ulb_loader, eval_loader)\n",
    "trainer.evaluate(eval_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "efd87a861e5021e4a438e5b61d692cea261dd91508182bfdfdb13fb969975ffe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
